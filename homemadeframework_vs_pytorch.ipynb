{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pair_sets():\n",
    "    data_dir = os.environ.get('PYTORCH_DATA_DIR')\n",
    "    if data_dir is None:\n",
    "        data_dir = './data'\n",
    "\n",
    "    train_set = datasets.MNIST(data_dir + '/mnist/', train = True, download = True)\n",
    "    train_features = train_set.train_data.view(-1, 1, 28, 28).float()\n",
    "    train_target = train_set.train_labels\n",
    "    train_features = torch.functional.F.avg_pool2d(train_features, kernel_size = 2)\n",
    "\n",
    "    test_set = datasets.MNIST(data_dir + '/mnist/', train = False, download = True)\n",
    "    test_features = test_set.test_data.view(-1, 1, 28, 28).float()\n",
    "    test_target = test_set.test_labels\n",
    "    test_features = torch.functional.F.avg_pool2d(test_features, kernel_size = 2)\n",
    "\n",
    "    return train_features, train_target, test_features, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "train_features, train_target, test_features, test_target = generate_pair_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 14, 14]) torch.Size([60000])\n",
      "torch.Size([10000, 1, 14, 14]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape, train_target.shape)\n",
    "print(test_features.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 1000\n",
    "train_perm = torch.randperm(train_features.size(0))\n",
    "idx = train_perm[:data_size]\n",
    "train_features = train_features[idx].reshape([data_size, train_features.size(2)**2])\n",
    "train_target = train_target[idx]\n",
    "\n",
    "test_perm = torch.randperm(test_features.size(0))\n",
    "idx = test_perm[:data_size]\n",
    "test_features = test_features[idx].reshape([data_size, test_features.size(2)**2])\n",
    "test_target = test_target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 196]) torch.Size([1000])\n",
      "torch.Size([1000, 196]) torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "def normalize(tensor):\n",
    "    mean, std = tensor.mean(), tensor.std()\n",
    "    return tensor.sub_(mean).div_(std)\n",
    "\n",
    "normalize(train_features)\n",
    "normalize(test_features)\n",
    "print(train_features.shape, train_target.shape)\n",
    "print(test_features.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displaying samples of data\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(test_features[i].reshape([14, 14]), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Value: {}\".format(test_target[i]))  \n",
    "    plt.tight_layout()\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train each model\n",
    "def train_model(model_, train_features_, train_target_, criterion_, optimizer_, num_epochs_, batch_size_, learning_rate_decay):\n",
    "    # getting start time of train to get the train time at the end thanks to \"end_time\"\n",
    "    start_time = datetime.datetime.now()\n",
    "    # list to get train and test errors at each epoch\n",
    "    train_error = []\n",
    "    test_error = []\n",
    "    # train function\n",
    "    # Learning rate decay can be enabled or disabled than to an input in the function's parameters\n",
    "    if learning_rate_decay:\n",
    "        lambda_ = lambda epoch: 0.8 ** epoch\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer_, lr_lambda=lambda_)\n",
    "    for epoch in range(1, num_epochs_+1):\n",
    "        # using technique of mini batch (size of the batch in the function's parameters)\n",
    "        for i in range(int(len(train_features_)/batch_size_)):  \n",
    "            # getting images and labels in right format\n",
    "            images = train_features_.narrow(0,i*batch_size_,batch_size_)\n",
    "            labels = train_target_.narrow(0,i*batch_size_,batch_size_)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model_(images)\n",
    "            loss = criterion_(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer_.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_.step()\n",
    "\n",
    "        if learning_rate_decay:\n",
    "            scheduler.step()\n",
    "        # getting train error at each epoch\n",
    "        train_error.append(test_accuracy(model_, train_features_, train_target_))\n",
    "        test_error.append(test_accuracy(model_, test_features, test_target))\n",
    "    # getting end time and training time\n",
    "    training_time = datetime.datetime.now() - start_time\n",
    "    print('Training time: {}'.format(training_time))\n",
    "    print('Loss: {:.4f} on final epoch: {}. Train error: {:.5f}%, Test error: {:.5f}%'.format(loss.item(),epoch,train_error[-1],test_error[-1]))\n",
    "    return train_error, test_error, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model_, my_test_features_, my_test_target_):\n",
    "    outputs = model_(my_test_features_)\n",
    "    _, predictions = torch.max(outputs.data, 1)\n",
    "    count_errors = (predictions.long() != my_test_target_.long()).sum().item()\n",
    "    return count_errors / my_test_features_.size(0) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homemade framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from homemade_framework import framework as NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epoch\n",
    "num_epochs = 200\n",
    "# batch size to compute mini-batch\n",
    "batch_size = 10\n",
    "# number of pixels in the image \n",
    "input_size = 196\n",
    "# number of possible digit: 0 to 9 \n",
    "num_class = 10\n",
    "# small step to find a minima\n",
    "learning_rate = 0.01\n",
    "# hidden size\n",
    "hidden_size = 128\n",
    "# p dropout\n",
    "p_dropout = 0\n",
    "# learning rate decay\n",
    "LRD = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fully connected neural network\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_class, dropout_p):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size) \n",
    "        self.ReLU = torch.nn.ReLU() \n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)  \n",
    "        self.fc3 = torch.nn.Linear(hidden_size, num_class) \n",
    "        self.batchNorm = torch.nn.BatchNorm1d(hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.ReLU(self.fc1(x))\n",
    "        x = self.batchNorm(x)\n",
    "        x = self.ReLU(self.fc2(x))\n",
    "        x = self.batchNorm(x)\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0:01:38.761933\n",
      "Loss: 1.4623 on final epoch: 200. Train error: 2.00000%, Test error: 9.80000%\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_class, p_dropout)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "train_error, test_error, train_time = train_model(model, train_features, train_target, criterion, optimizer, num_epochs, batch_size, LRD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train homemade model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model description: Linear in green, Activation in blue, Loss in magenta, Softmax in red, Flatten in Gray, Convolution in Cyan, BatchNormalization in Black, MaxPooling2D in Yellow, AveragePooling2D in highlight\n",
      "\u001b[32m\tLinear layer shape: [196, 128]\u001b[0m\n",
      "\u001b[34m\tLeakyReLU activation, a=0.01\u001b[0m\n",
      "\u001b[39m\tBatch normalization function: a=1, b=0\u001b[0m\n",
      "\u001b[32m\tLinear layer shape: [128, 128]\u001b[0m\n",
      "\u001b[34m\tLeakyReLU activation, a=0.01\u001b[0m\n",
      "\u001b[39m\tBatch normalization function: a=1, b=0\u001b[0m\n",
      "\u001b[32m\tLinear layer shape: [128, 10]\u001b[0m\n",
      "\u001b[31m\tSoftmax function\u001b[0m\n",
      "\u001b[35m\tMSE\u001b[0m\n",
      "Before training: Epoch: 0, Train Error: 88.8000%,        Test Error: 87.0000%, Loss  0.0000\n",
      "Epoch: 1, Train Error: 23.0000%,        Test Error: 26.2000%, Loss  70.1303\n",
      "Epoch: 51, Train Error: 4.2000%,        Test Error: 12.2000%, Loss  0.1054\n",
      "Epoch: 101, Train Error: 4.0000%,        Test Error: 12.0000%, Loss  0.0504\n",
      "Epoch: 151, Train Error: 3.9000%,        Test Error: 12.0000%, Loss  0.0336\n",
      "\n",
      "Training time: 0:00:18.524452\n",
      "After training: Epoch: 199, Train Error: 3.9000%,        Test Error: 11.7000%, Loss  0.0253\n"
     ]
    }
   ],
   "source": [
    "# convert data to numpy array\n",
    "train_features_np, train_target_np = train_features.numpy(), train_target.numpy()\n",
    "test_features_np, test_target_np = test_features.numpy(), test_target.numpy()\n",
    "\n",
    "# Build the model\n",
    "Model = NN.Sequential([NN.Linear(input_size, hidden_size), NN.LeakyReLU(), NN.BatchNorm(),\n",
    "                       NN.Linear(hidden_size, hidden_size), NN.LeakyReLU(), NN.BatchNorm(),\n",
    "                       NN.Linear(hidden_size, num_class), NN.Softmax()], NN.LossMSE())\n",
    "# Set the learning rate\n",
    "Model.set_Lr(learning_rate)\n",
    "# Print model's parameters\n",
    "Model.print(print_color=True)\n",
    "\n",
    "NN.train_homemade_model(Model, num_epochs, train_features_np, train_target_np, test_features_np, test_target_np, batch_size, print_every_n_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in results is less than 2% and the homemade model got trained 5.2 times faster, which may be due to the fact that my pytorch training function is not optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
