{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import os\n",
    "size = 1000\n",
    "\n",
    "def generate_pair_sets():\n",
    "    data_dir = os.environ.get('PYTORCH_DATA_DIR')\n",
    "    if data_dir is None:\n",
    "        data_dir = './data'\n",
    "\n",
    "    train_set = datasets.MNIST(data_dir + '/mnist/', train = True, download = True)\n",
    "    train_input = train_set.train_data.view(-1, 1, 28, 28).float()\n",
    "    train_target = train_set.train_labels\n",
    "    train_input = torch.functional.F.avg_pool2d(train_input, kernel_size = 2)\n",
    "\n",
    "    test_set = datasets.MNIST(data_dir + '/mnist/', train = False, download = True)\n",
    "    test_input = test_set.test_data.view(-1, 1, 28, 28).float()\n",
    "    test_target = test_set.test_labels\n",
    "    test_input = torch.functional.F.avg_pool2d(test_input, kernel_size = 2)\n",
    "    \n",
    "    return train_input, train_target, test_input, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antho\\Anaconda3\\envs\\venv_torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:55: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "C:\\Users\\antho\\Anaconda3\\envs\\venv_torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:45: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "C:\\Users\\antho\\Anaconda3\\envs\\venv_torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Users\\antho\\Anaconda3\\envs\\venv_torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "size = 1000\n",
    "train_features, train_target, test_features, test_target = generate_pair_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 14, 14]) torch.Size([60000])\n",
      "torch.Size([10000, 1, 14, 14]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape, train_target.shape)\n",
    "print(test_features.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 1000\n",
    "train_features = train_features[:nb].reshape([1000, 196])\n",
    "train_target = train_target[:nb].reshape([1000])\n",
    "test_features = test_features[:nb].reshape([1000, 196])\n",
    "test_target = test_target[:nb].reshape([1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displaying samples of data\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(test_features[i].reshape([14, 14]), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Value: {}\".format(test_target[i]))  \n",
    "    plt.tight_layout()\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fully connected neural network\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_class, dropout_p):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU() \n",
    "        self.softmax = nn.Softmax()\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)  \n",
    "        self.layer3 = nn.Linear(hidden_size, num_class) \n",
    "        self.dropout = nn.Dropout(dropout_p) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = self.layer1(x)\n",
    "        outputs = self.relu(outputs)\n",
    "        # outputs = self.layer2(outputs)\n",
    "        # outputs = self.dropout(outputs)\n",
    "        # outputs = self.relu(outputs)\n",
    "        outputs = self.layer3(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train each model\n",
    "def train_model(model_, train_features_, train_target_, criterion_, optimizer_, num_epochs_, batch_size_, learning_rate_decay):\n",
    "    # getting start time of train to get the train time at the end thanks to \"end_time\"\n",
    "    start_time = datetime.datetime.now()\n",
    "    # list to get train and test errors at each epoch\n",
    "    train_error = []\n",
    "    test_error = []\n",
    "    # train function\n",
    "    # Learning rate decay can be enabled or disabled than to an input in the function's parameters\n",
    "    if learning_rate_decay:\n",
    "        lambda_ = lambda epoch: 0.8 ** epoch\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer_, lr_lambda=lambda_)\n",
    "    for epoch in range(1, num_epochs_+1):\n",
    "        # using technique of mini batch (size of the batch in the function's parameters)\n",
    "        for i in range(int(len(train_features_)/batch_size_)):  \n",
    "            # getting images and labels in right format\n",
    "            images = train_features_.narrow(0,i*batch_size_,batch_size_)\n",
    "            labels = train_target_.narrow(0,i*batch_size_,batch_size_)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model_(images)\n",
    "            loss = criterion_(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer_.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_.step()\n",
    "\n",
    "        if learning_rate_decay:\n",
    "            scheduler.step()\n",
    "        # getting train error at each epoch\n",
    "        train_error.append(test_accuracy(model_, train_features_, train_target_))\n",
    "        test_error.append(test_accuracy(model_, test_features, test_target))\n",
    "    # getting end time and training time\n",
    "    end_time = datetime.datetime.now()\n",
    "    training_time = end_time - start_time\n",
    "    print ('Loss: {:.4f} on epoch: {}, train error: {:.5f}, test error: {:.5f}'.format(loss.item(),epoch,train_error[-1],test_error[-1]))\n",
    "    return train_error, test_error, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model_, my_test_input_, my_test_classes_):\n",
    "    total = my_test_input_.size(0)\n",
    "    outputs = model_(my_test_input_)\n",
    "    _, predictions = torch.max(outputs.data, 1)\n",
    "    well_predicted_count = (predictions.long() == my_test_classes_.long()).sum().item()\n",
    "\n",
    "    return 1 - well_predicted_count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.8020 on epoch: 30, train error: 0.72700, test error: 0.74400\n"
     ]
    }
   ],
   "source": [
    "# number of epoch\n",
    "num_epochs = 30\n",
    "# batch size to compute mini-batch\n",
    "batch_size = 100\n",
    "# number of pixels in the image \n",
    "input_size = 196\n",
    "# number of possible digit: 0 to 9 \n",
    "num_class = 10\n",
    "# small step to find a minima\n",
    "learning_rate = 0.004\n",
    "# hidden size\n",
    "hidden_size = 128\n",
    "# p dropout\n",
    "p_dropout = 0\n",
    "# learning rate decay\n",
    "LRD = False\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_class, p_dropout)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "train_error, test_error, train_time = train_model(model, train_features, train_target, criterion, optimizer, num_epochs, batch_size, LRD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homemade framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from homemade_framework import framework as NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model description: \n",
      "\tLinear layer shape: [196, 128]\n",
      "\tLeakyReLU activation\n",
      "\tLinear layer shape: [128, 10]\n",
      "\tLeakyReLU activation\n",
      "\tMSE\n",
      "Before training: Epoch: 0, Train Error: 97.1000%, Test Error: 96.4000%, Loss  0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antho\\Documents\\GitHub\\Pytorch_vs_homemadeFramework\\homemade_framework\\framework.py:174: RuntimeWarning: overflow encountered in matmul\n",
      "  return np.matmul(x, self.weight) + np.transpose(np.repeat(self.bias, x.shape[0], axis=1))\n",
      "C:\\Users\\antho\\Documents\\GitHub\\Pytorch_vs_homemadeFramework\\homemade_framework\\framework.py:100: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = np.multiply(neg.astype(float), x)*self.a + np.multiply(pos.astype(float), x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Error: 90.3000%, Test Error: 91.5000%, Loss  nan\n",
      "After training: Epoch: 29, Train Error: 90.3000%, Test Error: 91.5000%, Loss  nan\n"
     ]
    }
   ],
   "source": [
    "train_features_np, train_target_np = train_features.numpy(), train_target.numpy()\n",
    "test_features_np, test_target_np = test_features.numpy(), test_target.numpy()\n",
    "\n",
    "nb_epochs = 30\n",
    "batch_size = 1\n",
    "\n",
    "# Build the model\n",
    "Model = NN.Sequential([NN.Linear(196,128), NN.LeakyReLU(), NN.Linear(128,10), NN.LeakyReLU()], NN.LossMSE())\n",
    "# Set the learning rate\n",
    "Model.set_Lr(0.01)\n",
    "\n",
    "# Print model's parameters\n",
    "Model.print(print_color=False)\n",
    "\n",
    "# Convert train_target to one hot encoding\n",
    "train_target_one_hot = NN.convert_to_one_hot_labels(train_features, train_target)\n",
    "\n",
    "NN.print_current_results(0, Model, train_features_np, train_target_np, test_features_np, test_target_np, 0, prefix = \"Before training: \")\n",
    "test_results = []\n",
    "for epochs in range(0, nb_epochs):\n",
    "    loss_sum = 0\n",
    "    test_results.append(NN.get_inferences(Model, test_features_np))\n",
    "    for b in range(train_features.shape[0] // batch_size):\n",
    "        output = Model.forward(train_features_np[list(range(b * batch_size, (b+1) * batch_size))])\n",
    "        loss = Model.backward(train_target_one_hot[list(range(b * batch_size, (b+1) * batch_size))], output)\n",
    "        loss_sum = loss_sum + loss.item()\n",
    "    if epochs % 30 == 0:\n",
    "        NN.print_current_results(epochs + 1, Model, train_features_np, train_target_np, test_features_np, test_target_np, loss_sum)\n",
    "    \n",
    "        \n",
    "NN.print_current_results(epochs, Model, train_features_np, train_target_np, test_features_np, test_target_np, loss_sum, prefix = \"After training: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
