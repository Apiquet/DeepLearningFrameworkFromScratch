{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import os\n",
    "size = 1000\n",
    "\n",
    "def generate_pair_sets():\n",
    "    data_dir = os.environ.get('PYTORCH_DATA_DIR')\n",
    "    if data_dir is None:\n",
    "        data_dir = './data'\n",
    "\n",
    "    train_set = datasets.MNIST(data_dir + '/mnist/', train = True, download = True)\n",
    "    train_input = train_set.train_data.view(-1, 1, 28, 28).float()\n",
    "    train_target = train_set.train_labels\n",
    "    train_input = torch.functional.F.avg_pool2d(train_input, kernel_size = 2)\n",
    "\n",
    "    test_set = datasets.MNIST(data_dir + '/mnist/', train = False, download = True)\n",
    "    test_input = test_set.test_data.view(-1, 1, 28, 28).float()\n",
    "    test_target = test_set.test_labels\n",
    "    test_input = torch.functional.F.avg_pool2d(test_input, kernel_size = 2)\n",
    "    \n",
    "    return train_input, train_target, test_input, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000\n",
    "train_features, train_target, test_features, test_target = generate_pair_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 14, 14]) torch.Size([60000])\n",
      "torch.Size([10000, 1, 14, 14]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape, train_target.shape)\n",
    "print(test_features.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 1000\n",
    "train_features = train_features[:nb].reshape([1000, 196])\n",
    "train_target = train_target[:nb].reshape([1000])\n",
    "test_features = test_features[:nb].reshape([1000, 196])\n",
    "test_target = test_target[:nb].reshape([1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAELCAYAAADDZxFQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASxElEQVR4nO3cbYxUZZrG8eseG0VAZEBQ8AVEGbKIYhaUyQIBIoQgoGNURAdYRUnGl5AY/TBMoiI7GkTNIqLjrmGCiJCow8JGIDga0VXCREZHxWDAMDDoKO+20iIvw7Mfqnq3p+d+nqaKrqpT3f9fUolVV59z7qafePWpPnUshCAAABr7UaUHAABkEwUBAHBREAAAFwUBAHBREAAAFwUBAHC1yIIws15mFsysptKzIHtYH2gKayQnkwVhZmvNbLbz+rVm9nWWfmhmdrDR429m9nSl52rJqmV9mNlpZrbQzHaY2Xdm9qGZja30XK1BtawRSTKze8xso5kdNrNFlZ6noUwWhKRFkqaYmTV6fYqkl0IIx8o/ki+E0KH+IelsSYckvVLhsVq6RaqO9VEjaaek4ZLOlPSApJfNrFcFZ2otFqk61ogk/VXSryX9ttKDNJbVglghqbOkYfUvmNmPJY2XtDj/fFz+N7JvzWynmc2K7czMtpvZqAbPZ5nZkgbPf2pm683sGzP7yMxGFDn3DZJ2S/qfIrfHiamK9RFCqAshzAohbA8hHA8hvCbpz5IGFvbtoghVsUYkKYSwPISwQtK+Ar6/sshkQYQQDkl6WdLUBi9PlPRZCOGj/PO6fN5J0jhJd5rZzwo9lpmdK2mVcg3eWdL9kn5nZl3z+S/N7LUT3N2/SlocuH9JSVXr+jCzsyX9RNKnhc6BwlTrGsmaTBZE3guSbjSz0/PPp+ZfkySFENaFED7J/2b2saRlyp3KF2qypNUhhNX5ff1e0kZJV+ePMyeEML6pnZjZBfnjv9DU16JZVNv6aCPpJUkvhBA+K2IOFK6q1kgWZbYgQgjvStoj6Voz6y3pCklL63MzG2xmb5nZHjOrlfQLSWcVcaieyi2ib+ofkoZK6l7gfqZKejeE8OciZkCBqml9mNmPJL0o6Yike4qYAUWopjWSVZn5S37EYuX+x9tX0ushhF0NsqWSFkgaG0L4wczmKf7DrZPUrsHzcxr8905JL4YQpp/krFMlzTnJfaAwmV8f+T+SLlTuAoarQwhHi9kPipb5NZJlmT2DyFssaZSk6frHt27OkLQ//4O9UtItif38SdIkM2tjZoOU+2NyvSWSJpjZGDM7xczamtkIMzvvRIc0s3+RdK64eqncqmF9/EbSP0makH9fHOWV+TViZjVm1lbSKZLqt8/GL+8hhEw/JK2TdEDSaY1ev0HSDknfSXpNud8EluSzXpKCpJr8896S/iDpoHJ/TJpf/7X5fLCktyXtV+6UdJWkC/LZryStaWLG/1DuN4iK/3u1tkeW14dybz0EST/k913/+Hml/91a0yPLaySfz8ofq+FjVqX/3UIIsvyAAAD8nay/xQQAqBAKAgDgoiAAAC4KAgDgoiAAAK6CrrU1My55yqAQQuM7VlYE6yOz9oYQulZ6CIk1kmHuGuEMAmj5dlR6AGSeu0YoCACAi4IAALgoCACAi4IAALiyccdAoASGDh2azAcMGBDNjh6N35V75cqVyf3u2rUrmQPVgjMIAICLggAAuCgIAICLggAAuCgIAICLggAAuCgIAICLz0Ggqj3zzDPR7K677kpuu2XLlmjWpUuXaDZ48ODkfm+//fZkjpZv2bJlyfzxxx+PZh988EFzj1M0ziAAAC4KAgDgoiAAAC4KAgDgoiAAAC4KAgDgysRlrqNHj45m7du3j2YbNmxI7vfrr78ueiZkR69evaLZ1KlTo9moUaOS+123bl00mzdvXlHzoPVI/b+pT58+yW2zdClrCmcQAAAXBQEAcFEQAAAXBQEAcFEQAAAXBQEAcGXiMtfp06dHs9NPPz2ape7kKUnHjx+PZqeddlo0CyEk93vkyJFo9sYbb0Sz2267Lblf+LZv3x7NunfvHs0OHjyY3O/5558fzW699dZoNn/+/OR+0TqMGDEimq1fv758g5QQZxAAABcFAQBwURAAABcFAQBwURAAABcFAQBwZeIy14kTJxa1XU1NevxOnTpFs44dO0az1OWxkjRz5sxo1tQlsmheTV3KmpK6THrPnj3RbO7cuUUfEy3HlClTotl7771XxklKhzMIAICLggAAuCgIAICLggAAuCgIAICLggAAuCgIAIArE5+DKNaxY8eS+d69e4vK+vbtm9xv6vbkvXv3Tm6L8pkxY0YyHzNmTDQbMmRINKutrS16JlSXM888M5pdeeWV0ayptVctOIMAALgoCACAi4IAALgoCACAi4IAALgoCACAq6ovcy2VJ554Ipk/8sgj0Wz79u3NPA1SunTpEs1mz56d3HbhwoXRbOPGjUXPhJZj+PDh0Wz37t1FZdWEMwgAgIuCAAC4KAgAgIuCAAC4KAgAgIuCAAC4Wu1lrhdddFE0GzhwYHLbyZMnN/c4KNKjjz4azY4ePZrcdt68ec09DlqYm2++OZo9//zzZZykMjiDAAC4KAgAgIuCAAC4KAgAgIuCAAC4KAgAgIuCAAC4WvTnIGpq4t/eq6++Gs3mz5+f3G9tbW3RM6F53XHHHdFswYIFyW2/+OKLaNauXbtodujQoeR+27RpE83atm0bzYYNG5bcb8qaNWui2fHjx4veb2s3atSoaNYaPkfDGQQAwEVBAABcFAQAwEVBAABcFAQAwEVBAABcLfoy1wceeCCamVk0e+qpp0oxDkpg37590WzGjBnJbZvKYz788MNkfu6550azs846K5rt3bs3mn388cfJY+7YsSOabdq0KbltazZo0KBknvp5bd26tbnHyRzOIAAALgoCAOCiIAAALgoCAOCiIAAALgoCAOCq6stc+/Xrl8wffPDBaNa/f/9o1tTdOpEdAwcOjGYXXnhhctv27dtHs8GDB0ezr776KrnfzZs3R7ONGzdGs++//z65XzS/pu6gm7oj8P79+5t7nMzhDAIA4KIgAAAuCgIA4KIgAAAuCgIA4KIgAAAuCyGc+BebnfgXl8GKFSuS+TXXXBPNunfvHs127dpV9EyVEEKI35q2jLK2PvB//hhCSN+2tExYI5nlrhHOIAAALgoCAOCiIAAALgoCAOCiIAAALgoCAOCiIAAArqq+3ffKlSuT+dy5c6NZtX3WAQDKjTMIAICLggAAuCgIAICLggAAuCgIAICLggAAuAq93fceSTtKNw6K0DOE0LXSQ0isjwxjjaAp7hopqCAAAK0HbzEBAFwUBADARUEAAFwUBADARUEAAFwUBADARUEAAFwUBADARUEAAFwUBADARUEAAFwUBADARUEAAFwtsiDMrJeZBTOrqfQsyB7WB5rCGsnJZEGY2Vozm+28fq2ZfZ2lH5qZdTaz/zKzOjPbYWa3VHqmlq6a1kc9M+tjZj+Y2ZJKz9IaVNMaMbN7zGyjmR02s0WVnqehTBaEpEWSppiZNXp9iqSXQgjHyj9S1DOSjkg6W9LPJf3GzC6p7Egt3iJVz/qo94yk9ys9RCuySNWzRv4q6deSflvpQRrLakGskNRZ0rD6F8zsx5LGS1qcfz7OzD40s2/NbKeZzYrtzMy2m9moBs9nNfxNzsx+ambrzewbM/vIzEacyJBm1l7S9ZIeCCEcDCG8K+m/lVuEKJ2qWB8Ntp8k6RtJbxayHU5K1ayREMLyEMIKSfsK+P7KIpMFEUI4JOllSVMbvDxR0mchhI/yz+vyeSdJ4yTdaWY/K/RYZnaupFXKNXhnSfdL+p2Zdc3nvzSz1yKb/0TS30IIWxq89pEkziBKqIrWh8yso6TZku4r9NgoXjWtkSzLZEHkvSDpRjM7Pf98av41SVIIYV0I4ZMQwvEQwseSlkkaXsRxJktaHUJYnd/X7yVtlHR1/jhzQgjjI9t2kFTb6LVaSWcUMQcKUw3rQ5L+TdLCEMLOIo6Nk1MtaySzMlsQ+bdr9ki61sx6S7pC0tL63MwGm9lbZrbHzGol/ULSWUUcqqdyi+ib+oekoZK6n8C2ByV1bPRaR0nfFTEHClAN68PMLpc0StK/F3FcnKRqWCNZl5m/5EcsVq71+0p6PYSwq0G2VNICSWNDCD+Y2TzFf7h1kto1eH5Og//eKenFEML0IubbIqnGzPqEELbmXxsg6dMi9oXCZX19jJDUS9Jf8n8r7SDpFDPrF0L45yL2h8JlfY1kWmbPIPIWK/cb2HQ1ODXMO0PS/vwP9kpJqctL/yRpkpm1MbNBkm5okC2RNMHMxpjZKWbW1sxGmNl5TQ0XQqiTtFzSbDNrb2ZDJF0r6cUT/g5xMjK9PiT9p6SLJF2efzyn3HvVY07km0OzyPoakZnVmFlbSaco9wtEW8vKZbghhEw/JK2TdEDSaY1ev0HSDuXeznlNud8EluSzXpKCpJr8896S/qDcW0KrJM2v/9p8PljS25L2K3dKukrSBfnsV5LWJObrrNwVE3WS/iLplkr/m7WmR9bXR6OZZjXcLw/WSIN1ERo9ZlX63y2EIMsPCADA38n6W0wAgAqhIAAALgoCAOCiIAAALgoCAOAq6FpbM+OSpwwKITS+Y2VFsD4ya28IoWulh5BYIxnmrhHOIICWb0elB0DmuWuEggAAuCgIAICLggAAuCgIAICLggAAuCgIAICLggAAuCgIAICLggAAuCgIAICLggAAuCgIAICroLu5Vpt77703mj388MPR7PLLL0/ud9u2bUXPhPIZO3ZsMl+zZk1Jjtu5c+doVldXF80OHz5cinFQAddff30y37BhQzT78ssvm3uconEGAQBwURAAABcFAQBwURAAABcFAQBwURAAAFdVX+Z63XXXJfOHHnoomnXo0CGaDR8+PLnfffv2RbPa2trktmheZhbN5syZk9y2VJe5DhgwIJr17ds3mj333HOlGAcl0r1792i2dOnS5Lb33XdfNFuwYEHRMzU3ziAAAC4KAgDgoiAAAC4KAgDgoiAAAC4KAgDgyvxlrpdddlk0W758eXLbEEI0O3bsWDR79tlnk/udNm1aNBs2bFhyWzSvKVOmRLNNmzaVcZL/16lTp2h2ySWXlHESlNKdd94Zzdq0aZPctqnLYLOCMwgAgIuCAAC4KAgAgIuCAAC4KAgAgIuCAAC4MnGZa9u2baPZ008/Hc1Sl7FK0tatW6NZ6lLV1CWwkvT2229Hs/vvvz+aPfnkk9Gsqe+lNevWrVs0e/zxx6NZnz59SjFOk1Jr6/333y/jJCil1N2k165dm9x2//79zT1OSXAGAQBwURAAABcFAQBwURAAABcFAQBwURAAABcFAQBwZeJzEO3atYtmQ4cOjWavvPJKcr+TJ0+OZkePHm16sIiFCxdGs7lz5xa13YEDB4qep6WbNGlSNHv99dej2bfffluKcdS/f/9kPm7cuGg2derU5h4HJdSvX79olloHjz32WCnGKTvOIAAALgoCAOCiIAAALgoCAOCiIAAALgoCAODKxGWuPXr0iGZmFs1uuummUozTpNRMqezCCy+MZlzmGte1a9dotn79+jJOkpO6jFWSPvnkk2jGz7m6TJgwIZodPHgwmjV1CX614AwCAOCiIAAALgoCAOCiIAAALgoCAOCiIAAArkxc5jpy5MhoFkIo4yQ5M2fOTObTpk2LZu+88040+/TTT4ueqTVbtWpVNFu5cmU0Gz9+fNHH7NatWzQbNGhQcttly5YVfVxky+jRo6PZm2++Gc0OHz5cinHKjjMIAICLggAAuCgIAICLggAAuCgIAICLggAAuCgIAIArE5+DKPb22RdffHFyvzfeeGM0u+KKK6LZ2LFjk/s99dRTo9ndd98dzVrKtdHltmHDhmiWWgM9e/Ys+pibNm2KZk19nmXfvn1FHxfldd555yXzq666KppNnDixucfJHM4gAAAuCgIA4KIgAAAuCgIA4KIgAAAuCgIA4MrEZa6rV6+OZnPnzo1mW7ZsSe43davw1OWzu3fvTu53zpw50Sx1eSSa33fffRfNSvWzOHToUDKvq6sryXHR/Pr165fMDxw4EM1St5pvKTiDAAC4KAgAgIuCAAC4KAgAgIuCAAC4KAgAgCsTl7l+/vnn0WzIkCHRLHUJrCRdeuml0Wzt2rXR7KGHHkrud9u2bckcLVuPHj2S+ebNm8s0CU7WOeeck8z37NkTzY4cOdLc42QOZxAAABcFAQBwURAAABcFAQBwURAAABcFAQBwWeqOp//wxWYn/sUomxBC/Na0ZdRa1sfIkSOT+VtvvVWmSU7YH0MIgyo9hJS9NXLqqacm89Rl9hn8OZ8Md41wBgEAcFEQAAAXBQEAcFEQAAAXBQEAcFEQAAAXBQEAcPE5iBaAz0GgCXwOAk3hcxAAgBNHQQAAXBQEAMBFQQAAXBQEAMBFQQAAXDUFfv1eSTtKMQiK1rPSAzTA+sgm1gia4q6Rgj4HAQBoPXiLCQDgoiAAAC4KAgDgoiAAAC4KAgDgoiAAAC4KAgDgoiAAAC4KAgDg+l8MlBlxui1YRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displaying samples of data\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(test_features[i].reshape([14, 14]), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Value: {}\".format(test_target[i]))  \n",
    "    plt.tight_layout()\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First neural net\n",
    "Train the model on all the 2000 images in train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# number of epoch\n",
    "num_epochs = 25\n",
    "# batch size to compute mini-batch\n",
    "batch_size = 100\n",
    "# number of pixels in the image \n",
    "input_size = 196\n",
    "# number of possible digit: 0 to 9 \n",
    "num_class = 10\n",
    "# small step to find a minima\n",
    "learning_rate = 0.004\n",
    "# hidden size\n",
    "hidden_size = 500\n",
    "# p dropout\n",
    "p_dropout = 0\n",
    "\n",
    "# Fully connected neural network\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_class, dropout_p):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU() \n",
    "        self.softmax = nn.Softmax()\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)  \n",
    "        self.layer3 = nn.Linear(hidden_size, num_class) \n",
    "        self.dropout = nn.Dropout(dropout_p) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = self.layer1(x)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.layer2(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.layer3(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train each model\n",
    "def train_model(model_, my_train_input_, my_train_classes_, criterion_, optimizer_,num_epochs_,batch_size_,learning_rate_decay):\n",
    "    # getting start time of train to get the train time at the end thanks to \"end_time\"\n",
    "    start_time = datetime.datetime.now()\n",
    "    # list to get train and test errors at each epoch\n",
    "    train_error = []\n",
    "    test_error = []\n",
    "    # train function\n",
    "    # Learning rate decay can be enabled or disabled than to an input in the function's parameters\n",
    "    if learning_rate_decay:\n",
    "        lambda_ = lambda epoch: 0.8 ** epoch\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer_, lr_lambda=lambda_)\n",
    "    for epoch in range(1, num_epochs_+1):\n",
    "        # using technique of mini batch (size of the batch in the function's parameters)\n",
    "        for i in range(int(len(my_train_input_)/batch_size_)):  \n",
    "            # getting images and labels in right format\n",
    "            images = my_train_input_.narrow(0,i*batch_size_,batch_size_)\n",
    "            labels = my_train_classes_.narrow(0,i*batch_size_,batch_size_)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model_(images)\n",
    "            loss = criterion_(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer_.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_.step()\n",
    "\n",
    "        if learning_rate_decay:\n",
    "            scheduler.step()\n",
    "        # getting train error at each epoch\n",
    "        train_error.append(test_accuracy(model_, my_train_input_, my_train_classes_))\n",
    "        test_ = []\n",
    "        # getting test errors on 10 different samples\n",
    "        for i in range(1,10):\n",
    "            test_.append(test_accuracy(model_, test_input_10[i,:,:], test_classes_10[i,:]))\n",
    "        # getting the test error as the mean of the 10 we got\n",
    "        test_error.append(sum(test_) / len(test_))\n",
    "    # getting end time and training time\n",
    "    end_time = datetime.datetime.now()\n",
    "    training_time = end_time - start_time\n",
    "    print ('Loss: {:.4f} on epoch: {}, train error: {:.5f}, avg test error on 10 different samples: {:.5f}'.format(loss.item(),epoch,train_error[-1],test_error[-1]))\n",
    "    return train_error, test_error, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model_, my_test_input_, my_test_classes_):\n",
    "    total = my_test_input_.size(0)\n",
    "    outputs = model_(my_test_input_)\n",
    "    _, predictions = torch.max(outputs.data, 1)\n",
    "    well_predicted_count = (predictions.long() == my_test_classes_.long()).sum().item()\n",
    "\n",
    "    return 1 - well_predicted_count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1150 on epoch: 25, train error: 0.01950, avg test error on 10 different samples: 0.10633\n"
     ]
    }
   ],
   "source": [
    "batch_size, hidden_size, learning_rate, LRD, p_dropout, HS = 20, 500, 0.004, True, 0, 50\n",
    "\n",
    "model = NeuralNet(input_size, HS, num_class, p_dropout)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "train_error, test_error, train_time = train_model(model, my_train_input, my_train_classes, criterion, optimizer,num_epochs,batch_size,LRD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
