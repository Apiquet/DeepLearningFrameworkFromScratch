{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pair_sets():\n",
    "    data_dir = os.environ.get('PYTORCH_DATA_DIR')\n",
    "    if data_dir is None:\n",
    "        data_dir = './data'\n",
    "\n",
    "    train_set = datasets.MNIST(data_dir + '/mnist/', train = True, download = True)\n",
    "    train_features = train_set.train_data.view(-1, 1, 28, 28).float()\n",
    "    train_target = train_set.train_labels\n",
    "    train_features = torch.functional.F.avg_pool2d(train_features, kernel_size = 2)\n",
    "\n",
    "    test_set = datasets.MNIST(data_dir + '/mnist/', train = False, download = True)\n",
    "    test_features = test_set.test_data.view(-1, 1, 28, 28).float()\n",
    "    test_target = test_set.test_labels\n",
    "    test_features = torch.functional.F.avg_pool2d(test_features, kernel_size = 2)\n",
    "\n",
    "    return train_features, train_target, test_features, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_target, test_features, test_target = generate_pair_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 14, 14]) torch.Size([60000])\n",
      "torch.Size([10000, 1, 14, 14]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape, train_target.shape)\n",
    "print(test_features.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 1000\n",
    "train_perm = torch.randperm(train_features.size(0))\n",
    "idx = train_perm[:data_size]\n",
    "train_features = train_features[idx].reshape([data_size, train_features.size(2)**2])\n",
    "train_target = train_target[idx]\n",
    "\n",
    "test_perm = torch.randperm(test_features.size(0))\n",
    "idx = test_perm[:data_size]\n",
    "test_features = test_features[idx].reshape([data_size, test_features.size(2)**2])\n",
    "test_target = test_target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 196]) torch.Size([1000])\n",
      "torch.Size([1000, 196]) torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "def normalize(tensor):\n",
    "    mean, std = tensor.mean(), tensor.std()\n",
    "    return tensor.sub_(mean).div_(std)\n",
    "\n",
    "normalize(train_features)\n",
    "normalize(test_features)\n",
    "print(train_features.shape, train_target.shape)\n",
    "print(test_features.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displaying samples of data\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(test_features[i].reshape([14, 14]), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Value: {}\".format(test_target[i]))  \n",
    "    plt.tight_layout()\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Fully connected neural network\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_class, dropout_p):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size) \n",
    "        self.ReLU = torch.nn.ReLU() \n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)  \n",
    "        self.fc3 = torch.nn.Linear(hidden_size, num_class) \n",
    "        self.dropout = torch.nn.Dropout(dropout_p) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.ReLU(self.fc1(x)))\n",
    "        x = self.ReLU(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train each model\n",
    "def train_model(model_, train_features_, train_target_, criterion_, optimizer_, num_epochs_, batch_size_, learning_rate_decay):\n",
    "    # getting start time of train to get the train time at the end thanks to \"end_time\"\n",
    "    start_time = datetime.datetime.now()\n",
    "    # list to get train and test errors at each epoch\n",
    "    train_error = []\n",
    "    test_error = []\n",
    "    # train function\n",
    "    # Learning rate decay can be enabled or disabled than to an input in the function's parameters\n",
    "    if learning_rate_decay:\n",
    "        lambda_ = lambda epoch: 0.8 ** epoch\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer_, lr_lambda=lambda_)\n",
    "    for epoch in range(1, num_epochs_+1):\n",
    "        # using technique of mini batch (size of the batch in the function's parameters)\n",
    "        for i in range(int(len(train_features_)/batch_size_)):  \n",
    "            # getting images and labels in right format\n",
    "            images = train_features_.narrow(0,i*batch_size_,batch_size_)\n",
    "            labels = train_target_.narrow(0,i*batch_size_,batch_size_)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model_(images)\n",
    "            loss = criterion_(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer_.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_.step()\n",
    "\n",
    "        if learning_rate_decay:\n",
    "            scheduler.step()\n",
    "        # getting train error at each epoch\n",
    "        train_error.append(test_accuracy(model_, train_features_, train_target_))\n",
    "        test_error.append(test_accuracy(model_, test_features, test_target))\n",
    "    # getting end time and training time\n",
    "    training_time = datetime.datetime.now() - start_time\n",
    "    print('Training time: {}'.format(training_time))\n",
    "    print('Loss: {:.4f} on final epoch: {}. Train error: {:.5f}%, Test error: {:.5f}%'.format(loss.item(),epoch,train_error[-1],test_error[-1]))\n",
    "    return train_error, test_error, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model_, my_test_features_, my_test_target_):\n",
    "    outputs = model_(my_test_features_)\n",
    "    _, predictions = torch.max(outputs.data, 1)\n",
    "    count_errors = (predictions.long() != my_test_target_.long()).sum().item()\n",
    "    return count_errors / my_test_features_.size(0) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homemade framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from homemade_framework import framework as NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_homemade_model(model_, num_epochs_, train_features_np_, train_target_np_, batch_size_):\n",
    "    start_time = datetime.datetime.now()\n",
    "    # Convert train_target to one hot encoding\n",
    "    train_target_one_hot = NN.convert_to_one_hot_labels(train_features_np_, train_target_np_)\n",
    "\n",
    "    NN.print_current_results(0, model_, train_features_np_, train_target_np_, test_features_np, test_target_np, 0, prefix = \"Before training: \")\n",
    "    test_results = []\n",
    "    for epochs in range(0, num_epochs_):\n",
    "        loss_sum = 0\n",
    "        test_results.append(NN.get_inferences(Model, test_features_np))\n",
    "        for b in range(train_features.shape[0] // batch_size):\n",
    "            output = model_.forward(train_features_np_[list(range(b * batch_size_, (b+1) * batch_size_))])\n",
    "            loss = model_.backward(train_target_one_hot[list(range(b * batch_size_, (b+1) * batch_size_))], output)\n",
    "            loss_sum = loss_sum + loss.item()\n",
    "        if epochs % 30 == 0:\n",
    "            NN.print_current_results(epochs + 1, model_, train_features_np_, train_target_np_, test_features_np, test_target_np, loss_sum)\n",
    "\n",
    "    training_time = datetime.datetime.now() - start_time\n",
    "    print('\\nTraining time: {}'.format(training_time))\n",
    "    NN.print_current_results(epochs, Model, train_features_np, train_target_np, test_features_np, test_target_np, loss_sum, prefix = \"After training: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epoch\n",
    "num_epochs = 100\n",
    "# batch size to compute mini-batch\n",
    "batch_size = 10\n",
    "# number of pixels in the image \n",
    "input_size = 196\n",
    "# number of possible digit: 0 to 9 \n",
    "num_class = 10\n",
    "# small step to find a minima\n",
    "learning_rate = 0.01\n",
    "# hidden size\n",
    "hidden_size = 128\n",
    "# p dropout\n",
    "p_dropout = 0\n",
    "# learning rate decay\n",
    "LRD = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0:00:09.570388\n",
      "Loss: 0.0326 on final epoch: 100. Train error: 0.00000%, Test error: 11.50000%\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_class, p_dropout)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "train_error, test_error, train_time = train_model(model, train_features, train_target, criterion, optimizer, num_epochs, batch_size, LRD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train homemade model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model description: \n",
      "\tLinear layer shape: [196, 128]\n",
      "\tLeakyReLU activation\n",
      "\tLinear layer shape: [128, 10]\n",
      "\tLeakyReLU activation\n",
      "\tMSE\n",
      "Before training: Epoch: 0, Train Error: 86.1000%, Test Error: 86.6000%, Loss  0.0000\n",
      "Epoch: 1, Train Error: 91.1000%, Test Error: 92.2000%, Loss  1688.8074\n",
      "Epoch: 31, Train Error: 2.2000%, Test Error: 12.6000%, Loss  40.0427\n",
      "Epoch: 61, Train Error: 1.7000%, Test Error: 13.1000%, Loss  22.5703\n",
      "Epoch: 91, Train Error: 1.4000%, Test Error: 13.0000%, Loss  17.6478\n",
      "\n",
      "Training time: 0:00:03.192457\n",
      "After training: Epoch: 99, Train Error: 1.3000%, Test Error: 12.9000%, Loss  16.6798\n"
     ]
    }
   ],
   "source": [
    "# convert data to numpy array\n",
    "train_features_np, train_target_np = train_features.numpy(), train_target.numpy()\n",
    "test_features_np, test_target_np = test_features.numpy(), test_target.numpy()\n",
    "\n",
    "# Build the model\n",
    "Model = NN.Sequential([NN.Linear(input_size, hidden_size), NN.LeakyReLU(), NN.Linear(hidden_size, num_class), NN.LeakyReLU()], NN.LossMSE())\n",
    "# Set the learning rate\n",
    "Model.set_Lr(learning_rate)\n",
    "# Print model's parameters\n",
    "Model.print(print_color=False)\n",
    "\n",
    "train_homemade_model(Model, num_epochs, train_features_np, train_target_np, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
